# RicercaMente

## INTRODUCTION

RicercaMENTE is an open source project that aims to trace the history of data science and AI through the most important scientific researches published over the years.

For any doubts, advice or question, please contact the [founder](https://www.linkedin.com/in/pedrocchi-edoardo-129489243/)
## CONTRIBUTE

### How to contribute

1. Find a paper that is missing from our list and be sure of its relevance to data science and AI
2. Add your research and fill in all the fields in our table
3. You are a contributor

Attention! we are reconstructing history, so put the searches in chronological order within the table


### Understand the table

- Name = the name of the research
- Author =  the authors of the research
- Year = year of publication
- topic = the topic of the research (insert : AI, MATH, DATA)
- description = A brief description of the research, what is it about?
- Impact = why is it so important?
- Media = where was it published?
- Number of citations = do I have to explain it?
- Tier = enter **Tier 1** if the research completely revolutionised the world of data science or AI, or       enter **Tier 2** if the research made a major contribution but did not disrupt the community
- Link = link to read the research
- More = add whatever you want! this is the only non-compulsory data

## THE HISTORY
| Year | Name | Authors | Topic | Description | Impact | Media | Number of citations | Tier | Link | More |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1928 | On the Theory of Games of Strategy | John von Neumann | MATH | Introduced the concept of extensive-form games and the minimax theorem, establishing foundational principles for game theory. | Pioneering work that laid the groundwork for game theory, influencing diverse fields from economics to AI decision-making algorithms. | Contributions to the Theory of Games | - | 1 | [Link to Paper](https://cs.uwaterloo.ca/~y328yu/classics/vonNeumann.pdf) | |
| 1948 | A Mathematical Theory of Communication | Claude Shannon |PROGRAMMING| Proposed the fundamental concepts of information theory, including entropy, channel capacity, and the source coding theorem. Revolutionized the understanding of communication and laid the groundwork for data compression and error correction. | Pioneering work that significantly influenced information theory, data science, and communication systems. | The Bell System Technical Journal | - | 1 | [Link to Paper](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) | |
| 1950 | Computing Machinery and Intelligence | Alan Turing | AI | Introduced the concept of the Turing Test, a benchmark for determining a machine's ability to exhibit intelligent behavior indistinguishable from that of a human. | Pioneering work that laid the foundation for discussions on machine intelligence and artificial general intelligence (AGI). | Mind | - | 1 | [Link to Paper](https://academic.oup.com/mind/article/LIX/236/433/986238)|  |
| 1984 | Classification and Regression Trees | Leo Breiman | DATA | Introduced the fundamental concepts of decision trees, a method widely used in data science | Fundamental contribution that has shaped the approach to data analysis through decision trees. | Journal of the American Statistical Association | - | 1 | [Link to paper](https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman) | |
| 1997 | Long Short-Term Memory | Sepp Hochreiter and Jürgen Schmidhuber | AI | Introduces Long Short-Term Memory (LSTM), a type of Recurrent Neural Network (RNN) architecture designed to overcome the vanishing gradient problem in traditional RNN. | LSTM address the challenge of learning dependencies over extended time periods, leading to more effective and efficient training on sequential tasks. | Neural Computation | 97944 | 1 | [Link to paper](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) |  |
| 2009 | ImageNet Large Scale Visual Recognition Challenge | Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei | Visual Recognition, Challenge, Datasets | Provides a comprehensive overview of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in the field of object category classification and detection using large-scale datasets. The challenge has been conducted annually since 2010 and has garnered participation from over fifty institutions. | Advancements in Object Recognition, Large scale standardization for computer vision research datasets, Development of Deep Learning (Mostly CNNs) | Springer | 41444 | 1 | [Link to Paper](https://arxiv.org/pdf/1409.0575.pdf)|  |
| 2016 | Mastering the Game of Go with Deep Neural Networks and Tree Search | David Silver et al. | AI | Presented AlphaGo, a model that defeated the European Go champion by 5 games to 0. It uses [Monte Carlo Tree Search (MCTS)](https://hal.inria.fr/inria-00116992/document) to compute its next move, running simulations of possible outcomes. | Demonstrated that AI can tackle complex challenges and achieve excellence in strategic games | Nature | 17297 | 1 | [Link to Paper](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf) |  |
| 2017 | Attention is All You Need | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin | AI | Revealed the transformer, a new neural network that is a significant milestone in modern Deep Learning models. Shaped the way we think about and approach NLP problems. | Has had a profound impact on NLP research and applications | NIPS | 99204 | 1 | [Link to Paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) |  |
| 2021 | An Image is worth 16x16 words: Transformers for image recognition at scale | Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby | AI, Vision Transformers, Visual Recognition | This paper introduces ViT, a vision transformer that applies the Transformer architecture directly to sequences of image patches for image classification tasks. ViT achieves comparable or even superior performance to state-of-the-art convolutional networks while requiring less computational resources for training. This suggests that Transformers have the potential to revolutionize computer vision in the same way that they have revolutionized natural language processing. | Currently ViT's are used from image classification tasks to text-to-image generators (DAll-E 2, Stable Diffusion) | ICLR | 25855 | 1 | [Link to Paper](https://arxiv.org/pdf/2010.11929.pdf) |  |

